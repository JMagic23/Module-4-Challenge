{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\n",
    " ---\n",
    "\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P 500 Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp500_history.csv`: Contains historical closing prices of the S&P 500 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\equit\\AppData\\Local\\Temp\\ipykernel_41872\\545599946.py:3: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  whale_df = pd.read_csv(whale_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOROS FUND MANAGEMENT LLC</th>\n",
       "      <th>PAULSON &amp; CO.INC.</th>\n",
       "      <th>TIGER GLOBAL MANAGEMENT LLC</th>\n",
       "      <th>BERKSHIRE HATHAWAY INC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-03</th>\n",
       "      <td>-0.001266</td>\n",
       "      <td>-0.004981</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>-0.006569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-04</th>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>0.004213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-05</th>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-06</th>\n",
       "      <td>-0.007905</td>\n",
       "      <td>-0.003574</td>\n",
       "      <td>-0.008481</td>\n",
       "      <td>-0.013098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SOROS FUND MANAGEMENT LLC  PAULSON & CO.INC.   \\\n",
       "Date                                                        \n",
       "2015-03-02                        NaN                 NaN   \n",
       "2015-03-03                  -0.001266           -0.004981   \n",
       "2015-03-04                   0.002230            0.003241   \n",
       "2015-03-05                   0.004016            0.004076   \n",
       "2015-03-06                  -0.007905           -0.003574   \n",
       "\n",
       "            TIGER GLOBAL MANAGEMENT LLC  BERKSHIRE HATHAWAY INC  \n",
       "Date                                                             \n",
       "2015-03-02                          NaN                     NaN  \n",
       "2015-03-03                    -0.000496               -0.006569  \n",
       "2015-03-04                    -0.002534                0.004213  \n",
       "2015-03-05                     0.002355                0.006726  \n",
       "2015-03-06                    -0.008481               -0.013098  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading whale returns\n",
    "whale_path = Path(\"Resources/whale_returns.csv\")\n",
    "whale_df = pd.read_csv(whale_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "whale_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOROS FUND MANAGEMENT LLC      1\n",
       "PAULSON & CO.INC.              1\n",
       "TIGER GLOBAL MANAGEMENT LLC    1\n",
       "BERKSHIRE HATHAWAY INC         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls\n",
    "whale_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOROS FUND MANAGEMENT LLC      0\n",
       "PAULSON & CO.INC.              0\n",
       "TIGER GLOBAL MANAGEMENT LLC    0\n",
       "BERKSHIRE HATHAWAY INC         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop nulls\n",
    "whale_df_full = whale_df.dropna()\n",
    "whale_df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\equit\\AppData\\Local\\Temp\\ipykernel_41872\\3586740815.py:3: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  algo_df = pd.read_csv(algo_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algo 1</th>\n",
       "      <th>Algo 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-05-28</th>\n",
       "      <td>0.001745</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-29</th>\n",
       "      <td>0.003978</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-30</th>\n",
       "      <td>0.004464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-02</th>\n",
       "      <td>0.005692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-03</th>\n",
       "      <td>0.005292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Algo 1  Algo 2\n",
       "Date                        \n",
       "2014-05-28  0.001745     NaN\n",
       "2014-05-29  0.003978     NaN\n",
       "2014-05-30  0.004464     NaN\n",
       "2014-06-02  0.005692     NaN\n",
       "2014-06-03  0.005292     NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading algorithmic returns\n",
    "algo_path = Path(\"Resources/algo_returns.csv\")\n",
    "algo_df = pd.read_csv(algo_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "algo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algo 1    0\n",
       "Algo 2    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls\n",
    "algo_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algo 1    0\n",
       "Algo 2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop nulls\n",
    "algo_df_full = algo_df.dropna()\n",
    "algo_df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Returns\n",
    "\n",
    "Read the S&P 500 historic closing prices and create a new daily returns DataFrame from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\equit\\AppData\\Local\\Temp\\ipykernel_41872\\2977811894.py:3: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  sp500_df = pd.read_csv(sp500_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
      "C:\\Users\\equit\\AppData\\Local\\Temp\\ipykernel_41872\\2977811894.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  sp500_df = pd.read_csv(sp500_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>$2933.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>$2907.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>$2905.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>$2900.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>$2907.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close\n",
       "Date                \n",
       "2019-04-23  $2933.68\n",
       "2019-04-22  $2907.97\n",
       "2019-04-18  $2905.03\n",
       "2019-04-17  $2900.45\n",
       "2019-04-16  $2907.06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading S&P 500 Closing Prices\n",
    "sp500_path = Path(\"Resources/sp500_history.csv\")\n",
    "sp500_df = pd.read_csv(sp500_path, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "# added DateTimeIndex conversion to all 3 dataframes bc of encountered TypeError\n",
    "sp500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Close    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Data Types\n",
    "sp500_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fix Data Types\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m'''sp500_df['Close'] = sp500_df['Close'].astype('float64')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03msp500_df.dtypes'''\u001b[39;00m \u001b[38;5;66;03m# commented bc I can't turn close price to float yet without removing '$' symbol\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sp500_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sp500_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# replace $ with nothing\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sp500_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sp500_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m sp500_df\u001b[38;5;241m.\u001b[39mdtypes\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:190\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(data)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:244\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    241\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "# Fix Data Types\n",
    "'''sp500_df['Close'] = sp500_df['Close'].astype('float64')\n",
    "sp500_df.dtypes''' # commented bc I can't turn close price to float yet without removing '$' symbol\n",
    "\n",
    "sp500_df['Close'] = sp500_df['Close'].str.replace('$', '') # replace $ with nothing\n",
    "sp500_df['Close'] = sp500_df['Close'].astype('float64')\n",
    "sp500_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_returns_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate Daily Returns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m daily_returns_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_returns_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate Daily Returns\n",
    "daily_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_returns_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop nulls\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m aily_returns_df_full \u001b[38;5;241m=\u001b[39m daily_returns_df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      3\u001b[0m daily_returns_df_full\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_returns_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop nulls\n",
    "aily_returns_df_full = daily_returns_df.dropna()\n",
    "daily_returns_df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_returns_df_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Rename `Close` Column to be specific to this portfolio.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m daily_returns_df_full \u001b[38;5;241m=\u001b[39m daily_returns_df_full\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp500_return\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m daily_returns_df_full\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_returns_df_full' is not defined"
     ]
    }
   ],
   "source": [
    "# Rename `Close` Column to be specific to this portfolio.\n",
    "daily_returns_df_full = daily_returns_df_full.rename(columns={'Close': 'sp500_return'})\n",
    "daily_returns_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Whale, Algorithmic, and S&P 500 Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_returns_df_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Join Whale Returns, Algorithmic Returns, and the S&P 500 Returns into a single DataFrame with columns for each portfolio's returns.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([whale_df_full, algo_df_full, daily_returns_df_full], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m combined_df\u001b[38;5;241m.\u001b[39msort_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_returns_df_full' is not defined"
     ]
    }
   ],
   "source": [
    "# Join Whale Returns, Algorithmic Returns, and the S&P 500 Returns into a single DataFrame with columns for each portfolio's returns.\n",
    "combined_df = pd.concat([whale_df_full, algo_df_full, daily_returns_df_full], axis='columns', join='inner')\n",
    "combined_df.sort_index(inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Anlysis\n",
    "\n",
    "#### Calculate and Plot the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot daily returns of all portfolios\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot daily returns of all portfolios\n",
    "combined_df.plot(figsize=(10,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate cumulative returns of all portfolios\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cumulative_returns \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m combined_df)\u001b[38;5;241m.\u001b[39mcumprod()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot cumulative returns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cumulative_returns\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate cumulative returns of all portfolios\n",
    "cumulative_returns = (1 + combined_df).cumprod()\n",
    "# Plot cumulative returns\n",
    "cumulative_returns.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios\n",
    "4. Determine which portfolios are riskier than the S&P 500\n",
    "5. Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cumulative_returns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Box plot to visually show risk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cumulative_returns\u001b[38;5;241m.\u001b[39mboxplot(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m90\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cumulative_returns' is not defined"
     ]
    }
   ],
   "source": [
    "# Box plot to visually show risk\n",
    "cumulative_returns.boxplot(rot=-90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate the daily standard deviations of all portfolios\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m volatility \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m      3\u001b[0m volatility\u001b[38;5;241m.\u001b[39msort_values(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m volatility\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the daily standard deviations of all portfolios\n",
    "volatility = combined_df.std()\n",
    "volatility.sort_values(inplace=True)\n",
    "volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate  the daily standard deviation of S&P 500\n",
    "\"Completed above. 0.008587\"\n",
    "# Determine which portfolios are riskier than the S&P 500\n",
    "\"Since TIGER and BERKSHIRE have std greater than sp500, that means their fluctations prove them to be riskier.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\n",
    "volatility_ann = combined_df.std() * np.sqrt(252)\n",
    "volatility_ann.sort_values(inplace=True)\n",
    "volatility_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P 500\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta between it and the S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling standard deviation for all portfolios using a 21-day window\n",
    "whale_roll_std = whale_df_full.rolling(window=21).std()\n",
    "algo_roll_std = algo_df_full.rolling(window=21).std()\n",
    "sp500_roll_std = daily_returns_df_full.rolling(window=21).std()\n",
    "# Plot the rolling standard deviation\n",
    "ax = whale_roll_std.plot(figsize=(25,10))\n",
    "\n",
    "algo_roll_std.plot(ax = ax)\n",
    "sp500_roll_std.plot(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "correlation = combined_df.corr()\n",
    "# Display the correlation matrix\n",
    "sns.heatmap(correlation, vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for a chosen portfolio and the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1379580286.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# Calculate covariance of a single portfolio\n",
    "rolling_algo1_cov = combined_df['Algo 1'].rolling(window=60).cov(combined_df['sp500_return'])\n",
    ")\n",
    "\n",
    "# Calculate variance of S&P 500\n",
    "rolling_variance = combined_df['sp500_return'].rolling(window=60).var()\n",
    "\n",
    "# Computing beta\n",
    "beta = rolling_algo1_cov / rolling_variance\n",
    "\n",
    "# Plot beta trend\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(beta, color='blue', label='Beta')\n",
    "plt.title('Beta Trend')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Beta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half life for each portfolio, using standard deviation (`std`) as the metric of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_returns_df_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ewma_whale \u001b[38;5;241m=\u001b[39m whale_df_full\u001b[38;5;241m.\u001b[39mewm(halflife\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m      3\u001b[0m ewma_algo \u001b[38;5;241m=\u001b[39m algo_df_full\u001b[38;5;241m.\u001b[39mewm(halflife\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m----> 4\u001b[0m ewma_daily_returns \u001b[38;5;241m=\u001b[39m daily_returns_df_full\u001b[38;5;241m.\u001b[39mewm(halflife\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m      6\u001b[0m ax \u001b[38;5;241m=\u001b[39m ewma_whale\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      7\u001b[0m ewma_algo\u001b[38;5;241m.\u001b[39mplot(ax\u001b[38;5;241m=\u001b[39max)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_returns_df_full' is not defined"
     ]
    }
   ],
   "source": [
    "# Use `ewm` to calculate the rolling window\n",
    "ewma_whale = whale_df_full.ewm(halflife=21).std()\n",
    "ewma_algo = algo_df_full.ewm(halflife=21).std()\n",
    "ewma_daily_returns = daily_returns_df_full.ewm(halflife=21).std()\n",
    "\n",
    "ax = ewma_whale.plot(figsize=(20,10))\n",
    "ewma_algo.plot(ax=ax)\n",
    "ewma_daily_returns.plot(ax=ax)\n",
    "\n",
    "ax.legend(['Whale', 'Algo', 'S&P500'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\n",
    "average_return = combined_df.mean()\n",
    "risk_free_rate = 0.1\n",
    "sharpe_ratios = (average_return-risk_free_rate)/volatility_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sharpe_ratios' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the sharpe ratios as a bar plot\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sharpe_ratios\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSharpe Ratios\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sharpe_ratios' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "sharpe_ratios.plot.bar(title=\"Sharpe Ratios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P 500) and the whales portfolios.\n",
    "\n",
    "The algorithmic strategies demonstrate superior performance compared to select Whale portfolios. Notably, they outperform Paulson & Co. INC. and SOROS portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P 500. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 1st stock\n",
    "aapl_path = Path(\"Resources/aapl_historical.csv\")\n",
    "aapl_df = pd.read_csv(aapl_path, index_col='Trade DATE')\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 2nd stock\n",
    "cost_path = Path(\"Resources/cost_historical.csv\")\n",
    "cost_df = pd.read_csv(cost_path, index_col='Trade DATE')\n",
    "cost_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 3rd stock\n",
    "goog_path = Path(\"Resources/goog_historical.csv\")\n",
    "goog_df = pd.read_csv(goog_path, index_col='Trade DATE')\n",
    "goog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stocks in a single DataFrame\n",
    "all_stocks_df = pd.concat([aapl_df, cost_df, goog_df], axis='columns', join='inner')\n",
    "all_stocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset Date index\n",
    "all_stocks_df.index.name = 'Date'\n",
    "all_stocks_df.index = pd.to_datetime(all_stocks_df.index, infer_datetime_format=True)\n",
    "all_stocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize portfolio data by having a column per symbol\n",
    "all_stocks_df.columns = ['delete', 'AAPL', 'delete1', 'COST', 'delete2', 'GOOG']\n",
    "all_stocks_df.drop(columns=['delete','delete1','delete2'],inplace=True)\n",
    "all_stocks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "stocks_daily_returns = all_stocks_df.pct_change()\n",
    "stocks_daily_returns.head()\n",
    "\n",
    "# Drop NAs\n",
    "stocks_daily_returns = stocks_daily_returns.dropna()\n",
    "\n",
    "# Display sample data\n",
    "stocks_daily_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights\n",
    "weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "# Calculate portfolio return\n",
    "portfolio_returns = stocks_daily_returns.dot(weights)\n",
    "# Display sample data\n",
    "portfolio_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join your portfolio returns to the DataFrame that contains all of the portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join your returns DataFrame to the original returns DataFrame\n",
    "all_portfolios_df = pd.concat([combined_df, portfolio_returns], axis='columns', join='inner')\n",
    "all_portfolios_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\n",
    "all_portfolios_df = all_portfolios_df.dropna()\n",
    "all_portfolios_df.isnull().sum()\n",
    "all_portfolios_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the risk analysis with your portfolio to see how it compares to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized `std`\n",
    "new_ann_std = all_portfolios_df.std() * np.sqrt(252)\n",
    "new_ann_std.sort_values(inplace=True)\n",
    "new_ann_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation\n",
    "new_roll_std = stocks_daily_returns.rolling(window=21).std()\n",
    "\n",
    "# Plot rolling standard deviation\n",
    "new_roll_std.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the correlation\n",
    "new_corr = new_roll_std.corr()\n",
    "sns.heatmap(new_corr, vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Rolling 60-day Beta for Your Portfolio compared to the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot Beta\n",
    "rolling_stock_cov = portfolio_returns.rolling(window=60).cov(all_portfolios_df['sp500_return'])\n",
    "rolling_var = all_portfolios_df['sp500_return'].rolling(window=60).var()\n",
    "\n",
    "beta_rolling = rolling_stock_cov / rolling_var\n",
    "beta_rolling.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Annualized Sharpe Ratios\n",
    "\n",
    "# Calculate annualized volatility\n",
    "vol_ann = stocks_daily_returns.std() * np.sqrt(252)\n",
    "\n",
    "# Sort vol_ann values\n",
    "vol_ann.sort_values(inplace=True)\n",
    "\n",
    "# Calculate average return\n",
    "avg_return = portfolio_returns.mean()\n",
    "\n",
    "# Set risk-free rate\n",
    "risk_free_rate = 0.1  \n",
    "\n",
    "# Calculate Sharpe ratios\n",
    "sh_ratios = (avg_return - risk_free_rate) / vol_ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "sh_ratios.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your portfolio do?\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance of your portfolio, especially considering the similarity of your Sharpe ratios to those of Berkshire Hathaways holdings. This suggests that your portfolio is robust and well-constructed."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
